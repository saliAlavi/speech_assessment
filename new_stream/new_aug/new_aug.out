local
2023-12-07 16:45:11.878073: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-12-07 16:45:13.451174: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-07 16:45:13.451294: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-07 16:45:13.451884: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-07 16:45:13.933842: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/users/PAS2301/kibria5/.conda/envs/local/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
2023-12-07 16:45:24.648742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31141 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:86:00.0, compute capability: 7.0
2023-12-07 16:45:24.655394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 31141 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:af:00.0, compute capability: 7.0
2023-12-07 16:45:33.927654: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:553] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.
2.14.0
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]
Number of devices: 2
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 257, 1201, 1)]    0         
                                                                 
 resnet50 (Functional)       (None, 9, 38, 2048)       23581440  
                                                                 
 global_average_pooling2d (  (None, 2048)              0         
 GlobalAveragePooling2D)                                         
                                                                 
 dense (Dense)               (None, 2048)              4194304   
                                                                 
 re_lu (ReLU)                (None, 2048)              0         
                                                                 
 dense_1 (Dense)             (None, 2048)              4194304   
                                                                 
 re_lu_1 (ReLU)              (None, 2048)              0         
                                                                 
 dense_2 (Dense)             (None, 2048)              4196352   
                                                                 
=================================================================
Total params: 36166400 (137.96 MB)
Trainable params: 36113280 (137.76 MB)
Non-trainable params: 53120 (207.50 KB)
_________________________________________________________________
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 2048)]            0         
                                                                 
 dense_3 (Dense)             (None, 512)               1048576   
                                                                 
 re_lu_2 (ReLU)              (None, 512)               0         
                                                                 
 dense_4 (Dense)             (None, 2048)              1050624   
                                                                 
=================================================================
Total params: 2099200 (8.01 MB)
Trainable params: 2099200 (8.01 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
  0%|          | 0/50 [00:00<?, ?it/s]2023-12-07 16:46:17.883891: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8801
2023-12-07 16:46:19.373559: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8801
2023-12-07 16:46:28.859562: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2bae44cb7ac0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-12-07 16:46:28.859648: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2023-12-07 16:46:28.859661: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla V100-SXM2-32GB, Compute Capability 7.0
2023-12-07 16:46:29.234191: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
  2%|▏         | 1/50 [04:00<3:16:47, 240.96s/it]  4%|▍         | 2/50 [07:14<2:50:15, 212.83s/it]  6%|▌         | 3/50 [10:26<2:39:35, 203.73s/it]  8%|▊         | 4/50 [13:39<2:32:50, 199.37s/it] 10%|█         | 5/50 [16:52<2:27:38, 196.86s/it] 12%|█▏        | 6/50 [20:05<2:23:29, 195.66s/it] 14%|█▍        | 7/50 [23:20<2:20:02, 195.42s/it] 16%|█▌        | 8/50 [26:33<2:16:15, 194.64s/it] 18%|█▊        | 9/50 [29:47<2:12:48, 194.35s/it] 20%|██        | 10/50 [33:00<2:09:17, 193.93s/it] 22%|██▏       | 11/50 [36:12<2:05:50, 193.61s/it] 24%|██▍       | 12/50 [39:26<2:02:34, 193.55s/it] 26%|██▌       | 13/50 [42:40<1:59:29, 193.76s/it] 28%|██▊       | 14/50 [45:55<1:56:24, 194.02s/it] 30%|███       | 15/50 [49:09<1:53:17, 194.22s/it] 32%|███▏      | 16/50 [52:24<1:50:04, 194.25s/it] 34%|███▍      | 17/50 [55:38<1:46:51, 194.28s/it] 36%|███▌      | 18/50 [58:53<1:43:42, 194.44s/it] 38%|███▊      | 19/50 [1:02:07<1:40:28, 194.46s/it] 40%|████      | 20/50 [1:05:28<1:38:10, 196.35s/it] 42%|████▏     | 21/50 [1:08:43<1:34:43, 195.97s/it] 44%|████▍     | 22/50 [1:11:59<1:31:21, 195.78s/it] 46%|████▌     | 23/50 [1:15:14<1:28:05, 195.76s/it] 48%|████▊     | 24/50 [1:18:29<1:24:41, 195.46s/it] 50%|█████     | 25/50 [1:21:45<1:21:33, 195.73s/it] 52%|█████▏    | 26/50 [1:25:00<1:18:11, 195.48s/it] 54%|█████▍    | 27/50 [1:28:17<1:15:01, 195.74s/it] 56%|█████▌    | 28/50 [1:31:31<1:11:39, 195.44s/it] 58%|█████▊    | 29/50 [1:34:47<1:08:25, 195.48s/it] 60%|██████    | 30/50 [1:38:02<1:05:08, 195.43s/it] 62%|██████▏   | 31/50 [1:41:18<1:01:55, 195.58s/it] 64%|██████▍   | 32/50 [1:44:33<58:37, 195.39s/it]   66%|██████▌   | 33/50 [1:47:48<55:20, 195.33s/it] 68%|██████▊   | 34/50 [1:51:04<52:05, 195.37s/it] 70%|███████   | 35/50 [1:54:19<48:50, 195.38s/it] 72%|███████▏  | 36/50 [1:57:34<45:33, 195.27s/it] 74%|███████▍  | 37/50 [2:00:49<42:18, 195.26s/it] 76%|███████▌  | 38/50 [2:04:04<39:01, 195.14s/it] 78%|███████▊  | 39/50 [2:07:20<35:47, 195.20s/it] 80%|████████  | 40/50 [2:10:34<32:29, 194.94s/it] 82%|████████▏ | 41/50 [2:13:49<29:15, 195.00s/it] 84%|████████▍ | 42/50 [2:17:04<25:59, 194.99s/it] 86%|████████▌ | 43/50 [2:20:19<22:44, 194.91s/it] 88%|████████▊ | 44/50 [2:23:34<19:30, 195.13s/it] 90%|█████████ | 45/50 [2:26:49<16:14, 194.84s/it] 92%|█████████▏| 46/50 [2:30:04<12:59, 194.90s/it] 94%|█████████▍| 47/50 [2:33:19<09:44, 194.91s/it] 96%|█████████▌| 48/50 [2:36:33<06:29, 194.74s/it] 98%|█████████▊| 49/50 [2:39:49<03:15, 195.00s/it]100%|██████████| 50/50 [2:43:04<00:00, 195.11s/it]100%|██████████| 50/50 [2:43:04<00:00, 195.69s/it]
epoch: 1 loss: -0.892
epoch: 2 loss: -0.943
epoch: 3 loss: -0.959
epoch: 4 loss: -0.968
epoch: 5 loss: -0.973
epoch: 6 loss: -0.976
epoch: 7 loss: -0.978
epoch: 8 loss: -0.980
epoch: 9 loss: -0.982
epoch: 10 loss: -0.983
epoch: 11 loss: -0.984
epoch: 12 loss: -0.984
epoch: 13 loss: -0.985
epoch: 14 loss: -0.986
epoch: 15 loss: -0.986
epoch: 16 loss: -0.987
epoch: 17 loss: -0.987
epoch: 18 loss: -0.987
epoch: 19 loss: -0.988
epoch: 20 loss: -0.988
epoch: 21 loss: -0.988
epoch: 22 loss: -0.988
epoch: 23 loss: -0.988
epoch: 24 loss: -0.989
epoch: 25 loss: -0.989
epoch: 26 loss: -0.989
epoch: 27 loss: -0.989
epoch: 28 loss: -0.989
epoch: 29 loss: -0.989
epoch: 30 loss: -0.989
epoch: 31 loss: -0.990
epoch: 32 loss: -0.990
epoch: 33 loss: -0.990
epoch: 34 loss: -0.990
epoch: 35 loss: -0.990
epoch: 36 loss: -0.990
epoch: 37 loss: -0.990
epoch: 38 loss: -0.990
epoch: 39 loss: -0.990
epoch: 40 loss: -0.990
epoch: 41 loss: -0.990
epoch: 42 loss: -0.990
epoch: 43 loss: -0.991
epoch: 44 loss: -0.991
epoch: 45 loss: -0.991
epoch: 46 loss: -0.991
epoch: 47 loss: -0.991
epoch: 48 loss: -0.991
epoch: 49 loss: -0.991
epoch: 50 loss: -0.991
